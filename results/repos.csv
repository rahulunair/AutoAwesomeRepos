id,url,license,readme,stars_count,forks_count,pushed_at,updated_at,created_at,languages,topics,open_issues,closed_issues,description,fork,size,watchers_count,language,keyword,additional_keywords,is_relevant,brief_desc,class_label,fetch_data
24376854,https://github.com/libxsmm/libxsmm,Unknown License,libxsmm ! bsd 3 clause license https img.shields.io badge license bsd3 blue.svg bsd 3 clause license license.md ! gcc build status https badge.buildkite.com 2e962d4cfc7ddb10a6cd6c27b0d8033edf179a799e156cb363.svg?branch main gcc build status https github.com libxsmm libxsmm wiki status ! clang build status https badge.buildkite.com dafe7b363a2e66f7d5c9087f074f3eceb69b9aae4278202fd7.svg?branch main clang build status https github.com libxsmm libxsmm wiki status ! intel build status https badge.buildkite.com 63b5dc4095f460f1c011ae782f8e67ec0b8a6a9732d8abe3c7.svg?branch main intel build status https github.com libxsmm libxsmm wiki status ! mixed build status https badge.buildkite.com fad67b2fcad79e07ddfe9141974f360e9eca6223cd89e3593f.svg?branch main mixed build status https github.com libxsmm libxsmm wiki status ! static analysis status https scan.coverity.com projects 7405 badge.svg static analysis status https scan.coverity.com projects hfp libxsmm ! read the docs https readthedocs.org projects libxsmm badge ?version latest read the docs https libxsmm.readthedocs.io libxsmm is a library for specialized dense and sparse matrix operations as well as for deep learning primitives such as small convolutions. the library is targeting intel architecture with span intel 160 sse span span intel 160 avx span span intel 160 avx2 span span intel 160 avx 8209 512 span with vnni and bfloat16 and span intel 160 amx span advanced matrix extensions supported by future intel processor code named sapphire rapids. code generation is mainly based on span just 8209 in 8209 time jit span code specialization for compiler independent performance matrix multiplications matrix transpose copy sparse functionality and deep learning . libxsmm is suitable for build once and deploy everywhere i.e. no special target flags are needed to exploit the available performance. supported gemm datatypes are fp64 fp32 bfloat16 int16 and int8 . for a list questions and answers please also have a look at https github.com libxsmm libxsmm wiki q a https github.com libxsmm libxsmm wiki q a . where to go for documentation? readthedocs main https libxsmm.readthedocs.io and sample https libxsmm.readthedocs.io libxsmm samples documentation with full text search. pdf main https github.com libxsmm libxsmm raw main documentation libxsmm.pdf documentation file and separate sample https github.com libxsmm libxsmm raw main documentation libxsmm samples.pdf documentation. articles magazine article https software.intel.com sites default files parallel universe issue 34.pdf incl. sample code https github.com libxsmm libxsmm tree main samples magazine full list of articles articles . a name getting started a a name hello libxsmm a getting started the following c code is focused on a specific functionality but may be considered as hello libxsmm https github.com libxsmm libxsmm tree main samples hello . build the example with cd path to libxsmm make static 0 shared library save the code under hello.cpp below and compile with g i path to libxsmm include hello.cpp l path to libxsmm lib lxsmm lblas o hello gnu ccc and finally execute with ld library path path to libxsmm lib libxsmm verbose 2 . hello . plain c code https github.com libxsmm libxsmm blob main samples hello hello.c as well as fortran code https github.com libxsmm libxsmm blob main samples hello hello.f resemble the same example https github.com libxsmm libxsmm tree main samples hello . a name what is a small matrix multiplication a what is a small matrix multiplication? when characterizing the problem size by using the m n and k parameters a problem size suitable for libxsmm falls approximately within i m 160 n 160 k sup 1 3 sup 160 lt 160 64 i which illustrates that non square matrices or even tall and skinny shapes are covered as well . the library is typically used to generate code up to the specified threshold documentation libxsmm tune.md auto dispatch . raising the threshold may not only generate excessive amounts of code due to unrolling in m or k dimension but also miss to implement a tiling scheme to effectively utilize the cache hierarchy. for auto dispatched problem sizes above the configurable threshold explicitly jit ted code is not subject to the threshold libxsmm is falling back to blas. in terms of gemm the supported kernels are limited to alpha 1 beta 1 0 and transa n . a name what is a small convolution a what is a small convolution? in the last years new workloads such as deep learning and more specifically convolutional neural networks cnn emerged and are pushing the limits of today s hardware. one of the expensive kernels is a small convolution with certain kernel sizes such that calculations in the frequency space is not the most efficient method when compared with direct convolutions. libxsmm s current support for convolutions aims for an easy to use invocation of small direct convolutions which are intended for cnn training and classification. interfaces and domains a name interfaces a overview a name general interface a please have a look at https github.com libxsmm libxsmm tree main include https github.com libxsmm libxsmm tree main include for all published functions. get started with the following list of available domains and documented functionality mm matrix multiplication matrix multiplication tpp tensor processing primitives libxsmm tpp.md dnn deep neural networks deep neural networks aux service functions service functions perf performance performance be backend jit backend to initialize library internal resources an explicit initialization routine helps to avoid lazy initialization overhead when calling libxsmm for the first time. the library deallocates internal resources at program exit but also provides a companion of the afore mentioned initialization finalize . matrix multiplication a name interface for matrix multiplication a this domain mm supports small matrix multiplications smm batches of multiple multiplications as well as the industry standard interface for general matrix matrix multiplication gemm . the matrix multiplication domain mm documentation libxsmm mm.md contains routines for small tiled and parallelized matrix multiplications documentation libxsmm mm.md overview manual code dispatch customized matrix batches documentation libxsmm mm.md manual code dispatch batched multiplication explicit interface documentation libxsmm mm.md batched multiplication call wrapper static and dynamic linkage documentation libxsmm mm.md call wrapper deep learning a name interface for dl a the deep learning domain is detailed by the following sample codes https github.com libxsmm libxsmm tree main samples deeplearning . here we demonstrate how common operators in deep learning applications gemm with activation function fusion convolutions with activation function fusion various norming operators and pooling operators etc. can be implemented using the tensor processing primitive provided by libxsmm. example drivers for performance evaluation are provided as part of libxsmm dnn https github.com libxsmm libxsmm dnn tree main tests . service functions for convenient operation of the library and to ease integration some service routines are available. these routines may not belong to the core functionality of libxsmm smm or dnn domain but users are encouraged to use this domain aux . there are two categories span 1 160 routines span which are available for c and fortran and span 2 160 routines span that are only available per c interface. the service function domain aux documentation libxsmm aux.md contains routines for getting and setting the target architecture documentation libxsmm aux.md getting and setting the target architecture getting and setting the verbosity documentation libxsmm aux.md getting and setting the verbosity measuring time durations timer documentation libxsmm aux.md timer facility dispatching user data and multiple kernels documentation libxsmm aux.md user data dispatch loading and storing data i o documentation libxsmm aux.md meta image file io allocating memory documentation libxsmm aux.md memory allocation backend a name jit backend a more information about the jit backend and the code generator can be found in a separate document documentation libxsmm be.md . the encoder sample collection https github.com libxsmm libxsmm tree main samples encoder can help to get started writing a kernel using libxsmm. please note libxsmm s stand alone a name generator driver a generator driver documentation libxsmm be.md generator driver is considered legacy deprecated . build instructions overview the main interface file is generated and it is therefore not stored in the code repository. to inspect the interface for c c https github.com libxsmm libxsmm blob main src template libxsmm.h and fortran https github.com libxsmm libxsmm blob main src template libxsmm.f one can take a look at the template files used to generate the actual interface. there are two general ways to build and use libxsmm classic library abi classic library abi and link instructions link instructions c c and fortran header only header only c and c note libxsmm is available as prebuilt package for fedora redhat centos debian ubuntu freebsd and others. further libxsmm can be installed with the spack package manager https computing.llnl.gov projects spack hpc package manager or per easybuild easyconfig https github.com easybuilders . classic library abi there are two ways to rely on prebuilt code for a given project span 1 160 using span libxsmm s makefile based build system span 2 160 or span using another build system and writing own rules for building libxsmm rules for building libxsmm . the makefile based build system relies on span gnu 160 make span typically associated with the make command but e.g. freebsd is calling it gmake . the build can be customized by using span key 8209 value span pairs. span key 8209 value span pairs can be supplied in two ways span 1 160 after span the make command or span 2 160 prior span to the make command env which is effectively the same as exporting the span key 8209 value span pair as an environment variable export or setenv . both methods can be mixed the second method may require make s e flag . a name zero config abi a in contrast to header only zero config which does not require configuration by default 3rd party build systems can compile and link libxsmm s sources but still avoid configuring the library per libxsmm config.py . the prerequisite to omit configuration is to opt in by defining libxsmm default config d . the zero config feature is not available for libxsmm s fortran interface. note by default c c and fortran compilers are needed some sample code is written in c . beside of specifying the compilers make cxx g cc gcc fc gfortran and maybe ar ar the need for a fortran compiler can be relaxed make fc or make fortran 0 . the latter affects the availability of the module file and the corresponding libxsmm.f library the interface libxsmm.f is still generated . the build system considers a set of given key value pairs as a single unique build and triggers a rebuild for a distinct set of flags. for more advanced builds or additional background please consult the section about customization documentation libxsmm tune.md . to generate the interface of the library inside of the include directory and to build the static library by default static 1 is activated . run any or both of the following command s on cray systems the cray compiling environment cce should be used regardless of utilizing the cray compiler the intel compiler or the span gnu 160 compiler collection gcc span . the cce is eventually suppressing to build shared libraries static 0 . in any case span 1 160 switch span to the desired compiler module load switch and span 2 160 rely span on a variety of build environments is out of the box compatible see https github.com libxsmm libxsmm wiki compatibility https github.com libxsmm libxsmm wiki compatibility . if the build process is not successful it may help to avoid advanced gcc flags. this is useful with a tool chain which pretends to be gcc compatible and is treated as such but fails to consume the afore mentioned flags a name outdated binutils a in case of outdated binutils compilation can fail to assemble code when building the library this has nothing to do with jit generated code and it does not affect how jit code is targeting the system . libxsmm implements some functionality using compiler intrinsics and multiple code paths which are scheduled according to cpuid. in contrast to intrinsics 2 default intrinsics 1 enables a fully static code path according to the desired target. if no target is given e.g. avx 3 or avx 2 instruction set extensions cannot be leveraged for such code paths. try to fix failing compilation by building the latest gnu binutils and export path path to binutils bin path . binutils are versioned independently of span gnu 160 gcc span and other compilers. if one cannot update binutils work around with a cpuid value as tabulated in libxsmm cpuid.h https github.com libxsmm libxsmm blob main include libxsmm cpuid.h start at the upper end less than 1999 and decrement until compilation passes make intrinsics cpuid e.g. make intrinsics 1021 . as a last resort rely on a fully static code path to test and validate a build please consult https github.com libxsmm libxsmm wiki validation https github.com libxsmm libxsmm wiki validation . to run some basic sanity checks remember that each set of given key value pairs represents a different build and test to remove intermediate files or to remove all generated files and folders including the interface and the library archives run one of the make targets below. an additional distclean target recursively cleans the entire tree after span version 160 1.9 span . a name fortran a fortran code can make use of libxsmm by using the module and linking with libxsmmf libxsmm and libxsmmext a name header only fortran a by including libxsmm.f and linking with libxsmm and libxsmmext or by implicitly calling a subroutine and linking with libxsmm and libxsmmext . note libxsmmf requires libxsmmext starting with libxsmm 160 2.0 and thereby requires to link with the openmp runtime as well. using the fortran module or including the interface requires at least a span fortran 160 2003 span compiler f2k3 . span fortran 160 77 span compatibility is only implicitly available no interface and the available subset of routines is documented in libxsmm.f and marked with comments https github.com libxsmm libxsmm search?q implementation provided for fortran 77 compatibility part of the implementation . header only span version 160 1.4.4 span introduced support for header only usage in c and c . by only including libxsmm source.h allows to get around building the library. however this gives up on a clearly defined application binary interface abi . an abi may allow for hot fixes after deploying an application when relying on the shared library form and it may also ensure to only rely on the public interface of libxsmm. in contrast the header only form not only exposes the internal implementation of libxsmm but can also increase the turnaround time during development of an application due to longer compilation times . the header file is intentionally named libxsmm source .h since this header file relies on the src https github.com libxsmm libxsmm tree main src directory with the implications as noted earlier . a name zero config a the header only form depends on libxsmm source.h which is generated according to the content of the source folder src . span libxsmm 160 1.16 span and later provides header only support without invoking a make target zero configuration for any given checkout of libxsmm. to use configured header only non default libxsmm configured must be defined d . previously it was necessary to invoke make header only v1.6.2 or later make cheader prior to v1.6.2 or any target building the library make . the zero config feature allows 3rd party build systems an easier integration of libxsmm which also holds true if the system builds libxsmm from source see classic abi zero config abi . fortran code may include header only fortran libxsmm.f but still requires that interface to be generated. note building an application applies the same build settings to libxsmm! for instance to omit debug code inside of libxsmm ndebug must be defined dndebug . rules for building libxsmm libxsmm can be used as header only library i.e. no source code must be pre built. however it can be desirable to build libxsmm as an intermediate library using a custom setup or build system. the latter can still implement custom build rules to configure libxsmm s interface before building the code. more likely building libxsmm from source in a custom fashion can still be omitting to configure the interface and rely on zero config zero config abi i.e. defining libxsmm default config dlibxsmm default config . for example a cmake module for libxsmm can look like above libxsmm default config is propagated to dependent code public and further libxsmm is configured to not require a lapack blas library fallback d blas 0 . link instructions using the classic abi classic library abi including fortran fortran code requires linking libxsmm against the application. the library is agnostic with respect to the threading runtime and therefore an application is free to use any threading runtime e.g. openmp . the library is also thread safe and multiple application threads can call libxsmm s routines concurrently. enabling openmp for libxsmm s main library is supported as well omp 1 and mostly affects the synchronization primitives used inside of the library. all the omp functionality function postfix is served by the libxsmmext library which is automatically built with openmp enabled. when using this omp functionality libxsmmext needs to be present at the link line. a name table of libraries a library purpose libxsmm thread safe core functions same routine can be called concurrently . contains routines that can take a thread id and the number of library external threads. libxsmmf necessary when using the fortran module but not when including libxsmm.f or relying on implicit interfaces fortran 77 https github.com libxsmm libxsmm search?q implementation provided for fortran 77 compatibility . libxsmmext provides library internal openmp threaded functions carrying the omp postfix when compared to function name names of the core library. libxsmmnoblas supplies faked symbols for dgemm and others and thereby removes the need to link against a lapack blas library. a name pkg config a to ease linking with libxsmm pkg config can be used. for example similarly an application is free to choose any blas or lapack library if the link model available on the os supports this and therefore linking gemm routines when linking libxsmm itself by supplying blas 1 124 2 may prevent a user from making this decision at the time of linking the actual application. to use libxsmm without gemm related functionality any blas dependency can be removed in two ways span 1 160 building span a special library with make blas 0 or span 2 160 linking span the application against the libxsmmnoblas library. if an application however uses blas already the call wrapper documentation libxsmm mm.md call wrapper can be used to intercept existing blas calls and to rely on libxsmm instead . note libxsmm does not support to dynamically link libxsmm or libxsmmext so when blas is linked statically a . if blas is linked statically the static version of libxsmm must be used! installation there are two main mechanisms to install libxsmm both mechanisms can be combined span 1 160 building span the library in an span out 8209 of 8209 tree span fashion and span 2 160 installing span into a certain location. a name install build a building in an span out 8209 of 8209 tree span fashion looks like a name install prefix a installation into a specific location looks like prefix or destdir a name install destdir a both prefix and destdir are equivalent and can be relative or absolute paths. an installation can be repeated for different locations without triggering a rebuild. the prefix directory inside of each of the package configuration files pkg config is set to where libxsmm is built staging folder unless prefix or destdir is specified. the effect of prefix or destdir with respect to the pkg config files is independent of whether the install target is invoked or not make . further performing make install minimal omits the documentation default prefix share libxsmm . moreover pincdir poutdir pbindir and pdocdir allow to customize the locations underneath of the prefix location. to build a general package for an unpredictable audience linux distribution or similar it is advised to not over specify or customize the build step i.e. jit sse avx omp blas etc. should not be used. the following is building and installing a complete set of libraries where the generated interface matches both the static and the shared libraries runtime control a name running a handling errors the library handles errors with mechanisms available to the c programming language no exceptions . the backend uses result codes passed by an argument rather than an actual return value. such an argument is often a descriptor struct guiding and covering the state of the code generation. the frontend however may not hand out any error state which can be a big relief on the call side. instead the frontend implements a verbose mode verbose mode to inform about unexpected input or an error captured from the backend. guiding principles of libxsmm are muted operation by default non verbose and no unexpected exit from execution. verbose mode the verbose mode documentation libxsmm aux.md getting and setting the verbosity level of verbosity allows for an insight into the code dispatch mechanism by receiving a small tabulated statistic as soon as the library terminates. the design point for this functionality is to not impact the performance of any critical code path i.e. verbose mode is always enabled and does not require symbols sym 1 or debug code dbg 1 . the statistics appears stderr when the environment variable libxsmm verbose is set to a non zero value. for example the tables are distinct between single precision and double precision but either table is pruned if all counters are zero. if both tables are pruned the library shows the code path which would have been used for jit ting the code libxsmm target hsw otherwise the code path is shown in the table s header . the actual counters are collected for three buckets small kernels span mnk sup 1 3 sup 160 lt 160 13 span medium sized kernels span 13 160 lt 160 mnk sup 1 3 sup 160 lt 160 23 span and larger kernels span 23 160 lt 160 mnk sup 1 3 sup 160 lt 160 64 span the actual upper bound depends on libxsmm max mnk as selected at compile time . keep in mind that larger is supposedly still small in terms of arithmetic intensity which grows linearly with the kernel size . unfortunately the arithmetic intensity depends on the way a kernel is used which operands are loaded stored into main memory and it is not performance neutral to collect this information. the try counter represents all attempts to register statically generated kernels and all attempts to dynamically generate and register kernels. the try counter includes rejected jit requests due to unsupported gemm arguments. the jit and sta counters distinct the successful cases of the afore mentioned event try into dynamically jit and statically sta generated code. in case the capacity span o n 160 160 10 sup 5 sup span of the code registry is exhausted no more kernels can be registered although further attempts are not prevented. registering many kernels span o n 160 160 10 sup 3 sup span may ramp the number of hash key collisions col which can degrade performance. the latter is prevented if the small thread local cache is utilized effectively. since explicitly jit generated code libxsmm ?mmdispatch does not fall under the threshold criterion the above table is extended by one line if large kernels have been requested. this indicates a missing threshold criterion customized dispatch or asks for cache blocking the matrix multiplication. setting a verbosity level of at least two summarizes the number of registered jit generated kernels which includes the total size and counters for gemm mcopy matrix copy and tcopy matrix transpose kernels. if the call wrapper is used an additional runtime statistic becomes available see call wrapper documentation libxsmm mm.md call wrapper . a name objdump a note setting libxsmm verbose to a negative value dumps each generated jit kernel to a file binary with each file being named like the function name shown in intel vtune documentation libxsmm prof.md intelvtuneamplifier . disassembly of the raw binary files can be accomplished by call trace during the initial steps of employing the libxsmm api one may rely on a debug version of the library make dbg 1 . the latter also implies console output stderr in case of an error warning condition inside of the library. it is also possible to print the execution flow call trace inside of libxsmm can be combined with dbg 1 or opt 0 building an application which traces calls inside of the library requires the shared library of libxsmm alternatively the application is required to link the static library of libxsmm in a dynamic fashion gnu tool chain rdynamic . tracing calls without debugger can be then accomplished by an environment variable called libxsmm trace. syntactically up to three arguments separated by commas which allows to omit arguments are taken tid i n tid signifies the id of the thread to be traced with 1...nthreads being valid and where libxsmm trace 1 is filtering for the main thread in fact the first thread running into the trace facility grabbing all threads no filter can be achieved by supplying a negative id which is also the default when omitted . the second argument is pruning higher levels of the call tree with i 1 being the default level zero is the highest at the same level as the main function . the last argument is taking the number of inclusive call levels with n 1 being the default signifying no filter . although the ltrace linux utility provides similar insight the trace facility might be useful due to the afore mentioned filtering expressions. please note that the trace facility is severely impacting the performance even with libxsmm trace 0 and this is not just because of console output but rather since inlining internal functions might be prevented along with additional call overhead on each function entry and exit. therefore debug symbols can be also enabled separately make sym 1 implied by trace 1 or dbg 1 which might be useful when profiling an application. performance a name profiling a profiling an application which uses libxsmm s jit code is well supported. the library supports span intel 160 vtune 160 amplifier span and span linux 160 perf span . details are given on how to include profiler support and how to run the application. profiling using intel vtune amplifier documentation libxsmm prof.md intelvtuneamplifier profiling using linux perf documentation libxsmm prof.md linuxperf a name tuning a at build time a variety of options exist to customize libxsmm. the library is setup for a broad range of use cases which include sophisticated defaults for typical use. customizing performance documentation libxsmm tune.md tuning a name auto dispatch a tuning auto dispatch documentation libxsmm tune.md auto dispatch a name results a to find performance results of applications or performance reproducers the repository provides an orphaned branch called results which collects collateral material such as measured performance results along with explanatory figures. the results can be found at https github.com libxsmm libxsmm tree results libxsmm results https github.com libxsmm libxsmm tree results libxsmm results or the results can be cloned as shown below. please note that comparing performance results depends on whether the operands of the matrix multiplication are streamed or not. for example multiplying with all matrices covered by the l1 cache may have an emphasis towards an implementation which perhaps performs worse for the real workload if this real workload needs to stream some or all matrices from the main memory . most of the code samples https github.com libxsmm libxsmm tree main samples are aimed to reproduce performance results and it is encouraged to model the exact case or to look at real applications applications . applications high performance computing hpc b 1 160 b https cp2k.org https cp2k.org open source molecular dynamics and the dbcsr library https github.com cp2k dbcsr which processes batches of small matrix multiplications. the batches originate from a distributed block sparse matrix with problem specific small matrices. starting with cp2k 160 3.0 https www.cp2k.org version history libxsmm can substitute cp2k s libsmm library. b 2 160 b https github.com seissol seissol https github.com seissol seissol seissol is one of the leading codes for earthquake scenarios for simulating dynamic rupture processes. libxsmm provides highly optimized assembly kernels which form the computational back bone of seissol see https github.com tum i5 seissol kernels https github.com tum i5 seissol kernels . b 3 160 b https github.com nekbox nekbox https github.com nekbox nekbox nekbox is a highly scalable and portable spectral element code which is inspired by the nek5000 https nek5000.mcs.anl.gov code. nekbox is specialized for box geometries and intended to prototype new methods as well as to leverage fortran beyond the fortran 160 77 standard. libxsmm can be used to substitute the mxm std https github.com nek5000 nekbox blob box mxm std.f90 code. please also note libxsmm s nekbox reproducer https github.com libxsmm libxsmm tree main samples nek nek sample collection . b 4 160 b https github.com nek5000 nek5000 https github.com nek5000 nek5000 nek5000 is the open source highly scalable always portable spectral element code from https nek5000.mcs.anl.gov https nek5000.mcs.anl.gov . the development branch of the nek5000 code incorporates https github.com nek5000 nek5000 blob master core mxm wrapper.f libxsmm. b 5 160 b http pyfr.org http pyfr.org pyfr is an open source python based framework for solving advection diffusion type problems on streaming architectures by using the flux reconstruction approach. pyfr 160 1.6.0 optionally incorporates libxsmm http pyfr.org user guide.php as a matrix multiplication provider for the openmp backend. please also note libxsmm s pyfr related code sample https github.com libxsmm libxsmm tree main samples pyfr . b 6 160 b http dial3343.org about http dial3343.org about the extreme scale discontinuous galerkin environment edge is a solver for hyperbolic partial differential equations with emphasis on seismic simulations. the edge source code https github.com 3343 edge optionally relies on libxsmm but for high performance libxsmm s kernels are highly recommended. b 7 160 b https sxs collaboration.github.io spectre https sxs collaboration.github.io spectre spectre is an open source code for multi scale multi physics problems in astrophysics and gravitational physics which runs at petascale and is designed for exascale computers. in the future spectre may be applied to problems across discipline boundaries in fluid dynamics geoscience plasma physics nuclear physics and engineering. b 8 160 b https ceed.exascaleproject.org ceed code https ceed.exascaleproject.org ceed code the center for efficient exascale discretizations ceed is building on the efforts of the nek5000 mfem magma occa and petsc projects to develop application program interfaces apis both at high level and at low level to enable applications to take advantage of high order methods. the ceed low level api libceed https ceed.exascaleproject.org libceed uses libxsmm as a backend https github.com ceed libceed backends for high performance on cpus. b 9 160 b https github.com romeric fastor https github.com romeric fastor fastor is a lightweight high performance tensor algebra framework for modern c and can optionally use libxsmm as jit backend https github.com romeric fastor wiki 9. using the libxsmm mkl jit backend . machine learning ml b 10 160 b https github.com plaidml plaidml https github.com plaidml plaidml plaidml is an open source tensor compiler aiming for performance portability across a wide range of cpus gpus and other accelerators. combined with intel s ngraph compiler plaidml is targeting popular deep learning frameworks such as pytorch keras tensorflow and openvino. plaidml v1 https github.com plaidml plaidml tree plaidml v1 development branch adopted mlir https mlir.llvm.org an extensible compiler infrastructure gaining industry wide adoption. plaidml v1 started using libxsmm as backend for targeting cpus. b 11 160 b https github.com intel intel extension for pytorch https github.com intel intel extension for pytorch intel extension for pytorch aims for a smooth user experience of pytorch on cpus by the means of good performance. the extension pack started to rely on libxsmm for achieving high performance on cpus https arxiv.org abs 2005.04680 . b 12 160 b https github.com libxsmm tpp pytorch extension https github.com libxsmm tpp pytorch extension intel r tensor processing primitive extension for pytorch is an open source software library the integrates tensor processing primitives tpp https arxiv.org abs 2104.05755 into pytorch. it is aiming for a smooth user experience of pytorch on cpus by the means of good performance. intel s mlperf training submission codes leverage this project https github.com mlcommons training results v2.1 tree main intel benchmarks bert implementations pytorch cpu . b 13 160 b https github.com libxsmm libxsmm dnn https github.com libxsmm libxsmm dnn libxsmm dnn is an open source software library that demonstrates how tensor processing primitives tpp https arxiv.org abs 2104.05755 can be used to implement various deep learning primitives such as convolutions linear layers or even pooling and norming. due to the use of tpp not a single line of platform specific code is needed. automated driving ad b 15 160 b https software.seek.intel.com accelerating eigen math library https software.seek.intel.com accelerating eigen math library accelerating the eigen math library for automated driving workloads the need for speed in kalman filtering. an article in issue 160 31 https software.intel.com content www us en develop download parallel universe magazine issue 31 january 2018.html of the parallel universe magazine pdf https software.intel.com content dam develop public us en documents parallel universe issue 31.pdf . references b 1 160 b https sc19.supercomputing.org proceedings tech poster tech poster pages rpost244.html https sc19.supercomputing.org proceedings tech poster tech poster pages rpost244.html high performance deep learning via a single building block poster https sc19.supercomputing.org proceedings tech poster poster files rpost244s2 file2.pdf and abstract https sc19.supercomputing.org proceedings tech poster poster files rpost244s2 file3.pdf sc 19 the international conference for high performance computing networking storage and analysis denver colorado . b 2 160 b https dl.acm.org doi 10.1109 sc.2018.00069 https dl.acm.org doi 10.1109 sc.2018.00069 anatomy of high performance deep learning convolutions on simd architectures paper https arxiv.org pdf 1808.05567.pdf . sc 18 the international conference for high performance computing networking storage and analysis dallas texas . b 3 160 b https pasc17.pasc conference.org fileadmin user upload pasc17 program post116s2.pdf https pasc17.pasc conference.org fileadmin user upload pasc17 program post116s2.pdf dbcsr a sparse matrix multiplication library for electronic structure codes poster pasc 17 the pasc17 conference lugano switzerland . b 4 160 b https sc17.supercomputing.org sc17 20archive tech poster tech poster pages post190.html https sc17.supercomputing.org sc17 20archive tech poster tech poster pages post190.html understanding the performance of small convolution operations for cnn on intel architecture poster https sc17.supercomputing.org sc17 20archive tech poster poster files post190s2 file2.pdf and abstract https sc17.supercomputing.org sc17 20archive tech poster poster files post190s2 file3.pdf sc 17 the international conference for high performance computing networking storage and analysis denver colorado . b 5 160 b https www.computer.org csdl proceedings article sc 2016 8815a981 12omnceaq1d https www.computer.org csdl proceedings article sc 2016 8815a981 12omnceaq1d libxsmm accelerating small matrix multiplications by runtime code generation. sc 16 the international conference for high performance computing networking storage and analysis salt lake city utah . b 6 160 b http sc15.supercomputing.org sites all themes sc15images tech poster tech poster pages post137.html http sc15.supercomputing.org sites all themes sc15images tech poster tech poster pages post137.html libxsmm a high performance library for small matrix multiplications poster http sc15.supercomputing.org sites all themes sc15images tech poster poster files post137s2 file2.pdf and abstract http sc15.supercomputing.org sites all themes sc15images tech poster poster files post137s2 file3.pdf . sc 15 the international conference for high performance computing networking storage and analysis austin texas . b 7 160 b tensor processing primitives a programming abstraction for efficiency and portability in deep learning hpc workloads https arxiv.org abs 2104.05755 sc 21 the international conference for high performance computing networking storage and analysis st louis. articles b 1 160 b https www.nextplatform.com 2019 10 09 cloudy supercomputers join the hpc petascale club https www.nextplatform.com 2019 10 09 cloudy supercomputers join the hpc petascale club cloudy supercomputers join the hpc petascale club. an article written by rob farber 2019. the article covers libxsmm in a separate section. b 2 160 b https www.nextplatform.com 2019 06 26 counting the cost of scaling hpc applications https www.nextplatform.com 2019 06 26 counting the cost of scaling hpc applications counting the cost of scaling hpc applications. an article written by timothy prickett morgan 2019. this article is about cp2k open source molecular dynamics and not about libxsmm. however libxsmm was key for application performance. b 3 160 b https www.nextplatform.com 2019 06 26 counting the cost of scaling hpc applications https www.nextplatform.com 2019 06 26 counting the cost of scaling hpc applications azure benchmarks hc series across twenty thousand cores for hpc. an article written by john russell 2019. this article is about cp2k open source molecular dynamics and not about libxsmm. however libxsmm was key for application performance. b 4 160 b https software.intel.com sites default files parallel universe issue 34.pdf https software.intel.com content www us en develop download parallel universe magazine issue 34 october 2018.html libxsmm an open source based inspiration for hardware and software development at intel pdf https software.intel.com content dam develop public us en documents parallel universe issue 34.pdf . an article written by hans pabst greg henry and alexander heinecke 2018. b 5 160 b https medium.com rmfarber libxsmm brings deep learning lessons learned to many hpc applications 9143c6c93125 https medium.com rmfarber libxsmm brings deep learning lessons learned to many hpc applications 9143c6c93125 libxsmm brings deep learning lessons learned to many hpc applications. an article written by rob farber 2018. b 6 160 b https www.rdworldonline.com largest supercomputer simulation of sumatra andaman earthquake https www.rdworldonline.com largest supercomputer simulation of sumatra andaman earthquake largest supercomputer simulation of sumatra andaman earthquake. an article written by linda barney 2018.,715,174,2023-04-05 06:36:34,2023-04-02 23:50:50,2014-09-23 15:20:20,C (7316264)| Makefile (160932)| C++ (154937)| Shell (119065)| Fortran (96990)| Python (76774)| Batchfile (3459)| JavaScript (1077)| Starlark (882)| HTML (390)| CSS (242),jit|`simd|`avx512|`machine-learning|`sparse|`blas|`matrix-multiplication|`transpose|`bfloat16|`avx2|`avx|`sse|`vector|`intel|`matrix|`tensor|`convolution|`amx|`fortran,15,740,"Library for specialized dense and sparse matrix operations, and deep learning primitives.",0,308711,715,C,intel_extension_for_pytorch,,0, libxsmm is a library for specialized dense and sparse matrix operations as well as for deep learning primitives such as small convolutions. The library is suitable for build once and deploy everywhere i.e. No special target flags are needed to exploit the available performance,HPC,2023-04-05 07:52:19
58414589,https://github.com/oneapi-src/oneDNN,Apache License 2.0,oneapi deep neural network library onednn img align left src https spec.oneapi.io oneapi logo white scaled.jpg alt oneapi logo oneapi deep neural network library onednn is an open source cross platform performance library of basic building blocks for deep learning applications. onednn is part of oneapi https oneapi.io . the library is optimized for intel r architecture processors intel graphics and arm 64 bit architecture aarch64 based processors. onednn has experimental support for the following architectures nvidia gpu amd gpu openpower power isa ppc64 ibmz s390x and risc v. onednn is intended for deep learning applications and framework developers interested in improving application performance on intel cpus and gpus. deep learning practitioners should use one of the applications enabled with onednn applications enabled with onednn . table of contents documentation documentation installation installation system requirements system requirements applications enabled with onednn applications enabled with onednn support support contributing contributing license license security security trademark information trademark information documentation developer guide https oneapi src.github.io onednn explains programming model supported functionality and implementation details and includes annotated examples. api reference https oneapi src.github.io onednn group dnnl api.html provides a comprehensive reference of the library api. installation binary distribution of this software is available in anaconda https anaconda.org conda forge onednn intel oneapi https software.intel.com en us oneapi onednn the packages do not include library dependencies and these need to be resolved in the application at build time. see the system requirements system requirements section below and the build options https oneapi src.github.io onednn dev guide build options.html section in the developer guide https oneapi src.github.io onednn for more details on cpu and gpu runtimes. if the configuration you need is not available you can build the library from source https oneapi src.github.io onednn dev guide build.html . system requirements onednn supports platforms based on the following architectures intel 64 or amd64 https en.wikipedia.org wiki x86 64 arm 64 bit architecture aarch64 https developer.arm.com architectures cpu architecture a profile . openpower https openpowerfoundation.org ibm power isa https en.wikipedia.org wiki power isa . ibmz z architecture s390x https en.wikipedia.org wiki z architecture . risc v 64 bit rv64 https en.wikipedia.org wiki risc v . warning power isa ppc64 ibmz s390x and risc v rv64 support is experimental with limited testing validation. the library is optimized for the following cpus intel atom r processor at least intel sse4.1 support is required intel core tm processor at least intel sse4.1 support is required intel xeon r processor e3 e5 and e7 family formerly sandy bridge ivy bridge haswell and broadwell intel xeon scalable processor formerly skylake cascade lake cooper lake ice lake and sapphire rapids intel xeon cpu max series formerly sapphire rapids hbm future intel xeon scalable processor code name granite rapids on a cpu based on intel 64 or on amd64 architecture onednn detects the instruction set architecture isa at runtime and uses just in time jit code generation to deploy the code optimized for the latest supported isa. future isas may have initial support in the library disabled by default and require the use of run time controls to enable them. see cpu dispatcher control https oneapi src.github.io onednn dev guide cpu dispatcher control.html for more details. on a cpu based on arm aarch64 architecture onednn can be built with arm compute library integration. compute library is an open source library for machine learning applications and provides aarch64 optimized implementations of core functions. this functionality currently requires that compute library is downloaded and built separately see build from source https oneapi src.github.io onednn dev guide build.html . onednn only supports compute library versions 22.08 or later. warning on macos applications that use onednn may need to request special entitlements if they use the hardened runtime. see the linking guide https oneapi src.github.io onednn dev guide link.html for more details. the library is optimized for the following gpus intel processor graphics based on gen9 gen9.5 and gen11 and gen12 architectures intel iris r xe graphics formerly dg1 intel arc tm graphics formerly alchemist and dg2 intel data center gpu flex series formerly arctic sound m intel data center gpu max series formerly ponte vecchio requirements for building from source onednn supports systems meeting the following requirements operating system with intel 64 arm 64 power ibmz architecture support c compiler with c 11 standard support cmake https cmake.org download 2.8.12 or later arm compute library https github.com arm software computelibrary for builds using compute library on aarch64. the following tools are required to build onednn documentation doxygen http www.doxygen.nl download.html srcbin 1.8.5 or later doxyrest https github.com vovkos doxyrest 2.1.2 or later sphinx https www.sphinx doc.org en master usage installation.html 4.0.2 or later sphinx book theme https sphinx book theme.readthedocs.io en latest 0.0.41 or later configurations of cpu and gpu engines may introduce additional build time dependencies. cpu engine onednn cpu engine is used to execute primitives on intel architecture processors 64 bit arm architecture aarch64 processors 64 bit power isa ppc64 processors ibmz s390x and compatible devices. the cpu engine is built by default but can be disabled at build time by setting dnnl cpu runtime to none . in this case gpu engine must be enabled. the cpu engine can be configured to use the openmp tbb or sycl runtime. the following additional requirements apply openmp runtime requires c compiler with openmp 2.0 or later standard support tbb runtime requires threading building blocks tbb https www.threadingbuildingblocks.org 2017 or later. sycl runtime requires intel oneapi dpc c compiler https software.intel.com en us oneapi dpc compiler threading building blocks tbb https www.threadingbuildingblocks.org some implementations rely on openmp 4.0 simd extensions. for the best performance results on intel architecture processors we recommend using the intel c compiler. gpu engine intel processor graphics and xe architecture graphics are supported by the onednn gpu engine. the gpu engine is disabled in the default build configuration. the following additional requirements apply when gpu engine is enabled opencl runtime requires opencl runtime library opencl version 1.2 or later opencl driver with kernel language support for opencl c 2.0 or later with intel subgroups and usm extensions support sycl runtime requires intel oneapi dpc c compiler https software.intel.com en us oneapi dpc compiler opencl runtime library opencl version 1.2 or later oneapi level zero https github.com oneapi src level zero sycl runtime with nvidia gpu support requires oneapi dpc c compiler https github.com intel llvm nvidia cuda driver cublas 10.1 or later cudnn 7.6 or later sycl runtime with amd gpu support requires oneapi dpc c compiler https github.com intel llvm amd rocm https github.com radeonopencompute rocm version 5.3 or later miopen https github.com rocmsoftwareplatform miopen version 2.18 or later optional if amd rocm includes the required version of miopen rocblas https github.com rocmsoftwareplatform rocblas version 2.45.0 or later optional if amd rocm includes the required version of rocblas warning nvidia gpu support is experimental. general information build instructions and implementation limitations are available in the nvidia backend readme https github.com oneapi src onednn blob master src gpu nvidia readme.md . amd gpu support is experimental. general information build instructions and implementation limitations are available in the amd backend readme https github.com oneapi src onednn blob master src gpu amd readme.md . runtime dependencies when onednn is built from source the library runtime dependencies and specific versions are defined by the build environment. linux common dependencies gnu c library libc.so gnu standard c library v3 libstdc .so dynamic linking library libdl.so c math library libm.so posix threads library libpthread.so runtime specific dependencies runtime configuration compiler dependency dnnl cpu runtime omp gcc gnu openmp runtime libgomp.so dnnl cpu runtime omp intel c c compiler intel openmp runtime libiomp5.so dnnl cpu runtime omp clang intel openmp runtime libiomp5.so dnnl cpu runtime tbb any tbb libtbb.so dnnl cpu runtime sycl intel oneapi dpc compiler intel oneapi dpc compiler runtime libsycl.so tbb libtbb.so opencl loader libopencl.so dnnl gpu runtime ocl any opencl loader libopencl.so dnnl gpu runtime sycl intel oneapi dpc compiler intel oneapi dpc compiler runtime libsycl.so opencl loader libopencl.so oneapi level zero loader libze loader.so windows common dependencies microsoft visual c redistributable msvcrt.dll runtime specific dependencies runtime configuration compiler dependency dnnl cpu runtime omp microsoft visual c compiler no additional requirements dnnl cpu runtime omp intel c c compiler intel openmp runtime iomp5.dll dnnl cpu runtime tbb any tbb tbb.dll dnnl cpu runtime sycl intel oneapi dpc compiler intel oneapi dpc compiler runtime sycl.dll tbb tbb.dll opencl loader opencl.dll dnnl gpu runtime ocl any opencl loader opencl.dll dnnl gpu runtime sycl intel oneapi dpc compiler intel oneapi dpc compiler runtime sycl.dll opencl loader opencl.dll oneapi level zero loader ze loader.dll macos common dependencies system c c runtime libc .dylib libsystem.dylib runtime specific dependencies runtime configuration compiler dependency dnnl cpu runtime omp intel c c compiler intel openmp runtime libiomp5.dylib dnnl cpu runtime tbb any tbb libtbb.dylib validated configurations cpu engine was validated on redhat enterprise linux 7 with gnu compiler collection 4.8 5.4 6.1 7.2 8.1 and 9.1 clang 3.8.1 7.1 8.0 and 9.0 intel oneapi dpc c compiler https software.intel.com en us oneapi dpc compiler 2022.1 on windows server 2016 with microsoft visual studio 2019 and 2022 intel oneapi dpc c compiler https software.intel.com en us oneapi dpc compiler 2022.1 on macos 11 big sur with apple llvm version 13.0 intel oneapi dpc c compiler https software.intel.com en us oneapi dpc compiler 2022.1 gpu engine was validated on ubuntu 20.04 with gnu compiler collection 7.2 8.1 and 9.1 clang 3.8.1 7.1 8.0 and 9.0 intel oneapi dpc c compiler https software.intel.com en us oneapi dpc compiler 2022.1 intel software for general purpose gpu capabilities https dgpu docs.intel.com index.html latest stable version available at the time of release on windows server 2019 with microsoft visual studio 2019 and 2022 intel oneapi dpc c compiler https software.intel.com en us oneapi dpc compiler 2022.1 intel graphics windows 10 dch drivers https www.intel.com content www us en download 19344 intel graphics windows dch drivers.html intel arc graphics windows dch driver https www.intel.com content www us en download 726609 intel arc graphics windows dch driver.html latest stable version available at the time of release requirements for pre built binaries see the readme included in the corresponding binary package. applications enabled with onednn apache mxnet https mxnet.apache.org apache singa https singa.apache.org deeplearning4j https deeplearning4j.konduit.ai flashlight https github.com flashlight flashlight korali https github.com cselab korali matlab deep learning toolbox https www.mathworks.com help deeplearning onnx runtime https onnxruntime.ai openvino tm toolkit https github.com openvinotoolkit openvino paddlepaddle http www.paddlepaddle.org pytorch https pytorch.org . intel gpu support and additional optimizations are available with intel extension for pytorch https github.com intel intel extension for pytorch . tensorflow https www.tensorflow.org . intel gpu support and additional optimizations are available with intel extension for tensorflow https github.com intel intel extension for tensorflow . support please submit your questions feature requests and bug reports on the github issues https github.com oneapi src onednn issues page. you may reach out to project maintainers privately at dnnl.maintainers intel.com. contributing we welcome community contributions to onednn. if you have an idea on how to improve the library for changes impacting the public api or library overall such as adding new primitives or changes to the architecture submit an rfc pull request https github.com oneapi src onednn tree rfcs . ensure that the changes are consistent with the code contribution guidelines contributing.md code contribution guidelines and coding standards contributing.md coding standards . ensure that you can build the product and run all the examples with your patch. submit a pull request https github.com oneapi src onednn pulls . for additional details see contribution guidelines contributing.md . this project is intended to be a safe welcoming space for collaboration and contributors are expected to adhere to the contributor covenant code of conduct.md code of conduct. license onednn is licensed under apache license version 2.0 license . refer to the license license file for the full license text and copyright notice. this distribution includes third party software governed by separate license terms. 3 clause bsd license xbyak https github.com herumi xbyak gtest https github.com google googletest instrumentation and tracing technology api itt api https github.com intel ittapi cmake https github.com kitware cmake 2 clause bsd license sphinx https www.sphinx doc.org apache license version 2.0 xbyak aarch64 https github.com fujitsu xbyak aarch64 llvm https llvm.org boost software license version 1.0 boost c libraries https www.boost.org mit license intel graphics compute runtime for oneapi level zero and opencl driver https github.com intel compute runtime intel graphics compiler https github.com intel intel graphics compiler oneapi level zero https github.com oneapi src level zero doxyrest https github.com vovkos doxyrest intel metrics discovery application programming interface https github.com intel metrics discovery this third party software even if included with the distribution of the intel software may be governed by separate license terms including without limitation third party license terms other intel software license terms and open source software license terms. these separate license terms govern your use of the third party programs as set forth in the third party programs third party programs file. security see intel s security center https www.intel.com content www us en security center default.html for information on how to report a potential security issue or vulnerability. see also security policy security.md trademark information intel the intel logo arc intel atom intel core iris openvino the openvino logo pentium vtune and xeon are trademarks of intel corporation or its subsidiaries. other names and brands may be claimed as the property of others. microsoft windows and the windows logo are trademarks or registered trademarks of microsoft corporation in the united states and or other countries. opencl and the opencl logo are trademarks of apple inc. used by permission by khronos. c intel corporation,3111,870,2023-04-04 19:07:58,2023-04-04 23:21:21,2016-05-09 23:26:42,C++ (38923382)| C (3951660)| Python (576905)| CMake (306916)| Assembly (22181)| Shell (1241),onednn|`oneapi|`deep-learning|`deep-neural-networks|`performance|`cpp|`openmp|`tbb|`x86-64|`x64|`aarch64|`avx512|`amx|`xe-architecture|`library|`bfloat16|`sycl|`vnni,44,1553,oneAPI Deep Neural Network Library (oneDNN),0,128114,3111,C++,intel_extension_for_pytorch,,0, Oneapi deep neural network library onednn is an open source cross platform performance library of basic building blocks for deep learning applications. The library is optimized for intel r architecture processors intel graphics and arm 64 bit architecture aarch64 based processors,Tools & Development,2023-04-05 07:52:19
256061008,https://github.com/intel/intel-extension-for-pytorch,Unknown License,intel extension for pytorch intel extension for pytorch extends pytorch with up to date features optimizations for an extra performance boost on intel hardware. optimizations take advantage of avx 512 vector neural network instructions avx512 vnni and intel advanced matrix extensions intel amx on intel cpus as well as intel x sup e sup matrix extensions xmx ai engines on intel discrete gpus. moreover through pytorch xpu device intel extension for pytorch provides easy gpu acceleration for intel discrete gpus with pytorch . intel extension for pytorch provides optimizations for both eager mode and graph mode however compared to eager mode graph mode in pytorch normally yields better performance from optimization techniques such as operation fusion. intel extension for pytorch amplifies them with more comprehensive graph optimizations. therefore we recommend you to take advantage of intel extension for pytorch with torchscript https pytorch.org docs stable jit.html whenever your workload supports it. you could choose to run with torch.jit.trace function or torch.jit.script function but based on our evaluation torch.jit.trace supports more workloads so we recommend you to use torch.jit.trace as your first choice. the extension can be loaded as a python module for python programs or linked as a c library for c programs. in python scripts users can enable it dynamically by importing intel extension for pytorch . check cpu tutorial https intel.github.io intel extension for pytorch cpu latest for detailed information of intel extension for pytorch for intel cpus. source code is available at the master branch https github.com intel intel extension for pytorch tree master . check gpu tutorial https intel.github.io intel extension for pytorch xpu latest for detailed information of intel extension for pytorch for intel gpus. source code is available at the xpu master branch https github.com intel intel extension for pytorch tree xpu master . installation cpu version you can use either of the following 2 commands to install intel extension for pytorch cpu version. note intel extension for pytorch has pytorch version requirement. please check more detailed information via the url below. more installation methods can be found at cpu installation guide https intel.github.io intel extension for pytorch cpu latest tutorials installation.html . compilation instruction of the latest cpu code base master branch can be found at installation guide https github.com intel intel extension for pytorch blob master docs tutorials installation.md install via compiling from source . gpu version you can install intel extension for pytorch for gpu via command below. note the patched pytorch 1.13.0a0 is required to work with intel extension for pytorch on intel graphics card for now. more installation methods can be found at gpu installation guide https intel.github.io intel extension for pytorch xpu latest tutorials installation.html . compilation instruction of the latest gpu code base xpu master branch can be found at installation guide https github.com intel intel extension for pytorch blob xpu master docs tutorials installation.md install via compiling from source . getting started minor code changes are required for users to get start with intel extension for pytorch . both pytorch imperative mode and torchscript mode are supported. you just need to import intel extension for pytorch package and apply its optimize function against the model object. if it is a training workload the optimize function also needs to be applied against the optimizer object. the following code snippet shows an inference code with fp32 data type. more examples on cpu including training and c examples are available at cpu example page https intel.github.io intel extension for pytorch cpu latest tutorials examples.html . more examples on gpu are available at gpu example page https intel.github.io intel extension for pytorch xpu latest tutorials examples.html . inference on cpu inference on gpu model zoo use cases that had already been optimized by intel engineers are available at model zoo for intel architecture https github.com intelai models tree pytorch r2.0 models . a bunch of pytorch use cases for benchmarking are also available on the github page https github.com intelai models tree pytorch r2.0 models benchmarks pytorch use cases . you can get performance benefits out of box by simply running scipts in the model zoo. license apache license version 2.0 . as found in license https github.com intel intel extension for pytorch blob master license.txt file. security see intel s security center https www.intel.com content www us en security center default.html for information on how to report a potential security issue or vulnerability. see also security policy security.md,705,96,2023-04-03 06:17:57,2023-04-04 16:23:47,2020-04-15 23:35:29,C++ (2108558)| Python (2061147)| C (245132)| CMake (55466)| Shell (13263),pytorch|`neural-network|`machine-learning|`deep-learning|`intel|`quantization,61,261,A Python package for extending the official PyTorch that can easily obtain performance on Intel platform,0,52130,705,C++,intel_extension_for_pytorch,,0, intel extension for pytorch has up to date features optimized for an extra performance boost on intel hardware. optimizations take advantage of avx 512 vector neural network instructions avx512 vnni and intel advanced matrix extensions intel amx on intel cpus as well as intel x sup e sup matrix extensions xmx ai engines on intel discrete gpus,AI - Computer Vision,2023-04-05 07:52:19
281528773,https://github.com/intel/neural-compressor,Apache License 2.0,div align center intel neural compressor h3 an open source python library supporting popular model compression techniques on all mainstream deep learning frameworks tensorflow pytorch onnx runtime and mxnet h3 ! python https img.shields.io badge python 3.7 2b blue https github.com intel neural compressor ! version https img.shields.io badge release 2.1 green https github.com intel neural compressor releases ! license https img.shields.io badge license apache 202 blue https github.com intel neural compressor blob master license ! coverage https img.shields.io badge coverage 85 25 green https github.com intel neural compressor ! downloads https static.pepy.tech personalized badge neural compressor?period total units international system left color grey right color green left text downloads https pepy.tech project neural compressor architecture . docs source design.md architecture nbsp nbsp nbsp nbsp nbsp nbsp workflow . docs source design.md workflow nbsp nbsp nbsp nbsp nbsp nbsp results . docs source validated model list.md nbsp nbsp nbsp nbsp nbsp nbsp examples . examples readme.md nbsp nbsp nbsp nbsp nbsp nbsp documentations https intel.github.io neural compressor div div align left intel neural compressor aims to provide popular model compression techniques such as quantization pruning sparsity distillation and neural architecture search on mainstream frameworks such as tensorflow https www.tensorflow.org pytorch https pytorch.org onnx runtime https onnxruntime.ai and mxnet https mxnet.apache.org as well as intel extensions such as intel extension for tensorflow https github.com intel intel extension for tensorflow and intel extension for pytorch https github.com intel intel extension for pytorch . in particular the tool provides the key features typical examples and open collaborations as below support a wide range of intel hardware such as intel xeon scalable processor https www.intel.com content www us en products details processors xeon scalable.html intel xeon cpu max series https www.intel.com content www us en products details processors xeon max series.html intel data center gpu flex series https www.intel.com content www us en products details discrete gpus data center gpu flex series.html and intel data center gpu max series https www.intel.com content www us en products details discrete gpus data center gpu max series.html with extensive testing support amd cpu arm cpu and nvidia gpu through onnx runtime with limited testing validate more than 10 000 models such as bloom 176b examples pytorch nlp huggingface models language modeling quantization ptq static ipex smooth quant opt 30b examples pytorch nlp huggingface models language modeling quantization ptq static ipex smooth quant stable diffusion examples pytorch nlp huggingface models text to image quantization gpt j examples pytorch nlp huggingface models language modeling quantization ptq static fx bert large examples pytorch nlp huggingface models text classification quantization ptq static fx and resnet50 examples pytorch image recognition torchvision models quantization ptq cpu fx from popular model hubs such as hugging face https huggingface.co torch vision https pytorch.org vision stable index.html and onnx model zoo https github.com onnx models models by leveraging zero code optimization solution neural coder neural coder what do we offer and automatic accuracy driven docs source design.md workflow quantization strategies collaborate with cloud marketplace such as google cloud platform https console.cloud.google.com marketplace product bitnami launchpad inc tensorflow intel?project verdant sensor 286207 amazon web services https aws.amazon.com marketplace pp prodview yjyh2xmggbmga pdp support and azure https azuremarketplace.microsoft.com en us marketplace apps bitnami.inc tensorflow intel software platforms such as alibaba cloud https www.intel.com content www us en developer articles technical quantize ai by oneapi analytics on alibaba cloud.html and tencent taco https new.qq.com rain a 20221202a00b9s00 and open ai ecosystem such as hugging face https huggingface.co blog intel pytorch https pytorch.org tutorials recipes intel neural compressor for pytorch.html onnx https github.com onnx models models and lightning ai https github.com lightning ai lightning blob master docs source pytorch advanced post training quantization.rst installation install from pypi more installation methods can be found at installation guide . docs source installation guide.md . please check out our faq . docs source faq.md for more details. getting started quantization with python api more quick samples can be found in get started page . docs source get started.md . documentation table class docutils thead tr th colspan 9 overview th tr thead tbody tr td colspan 3 align center a href . docs source design.md architecture architecture a td td colspan 2 align center a href . docs source design.md workflow workflow a td td colspan 2 align center a href https intel.github.io neural compressor latest docs source api doc apis.html apis a td td colspan 2 align center a href . docs source bench.md gui a td tr tr td colspan 2 align center a href examples readme.md notebook examples notebook a td td colspan 2 align center a href examples readme.md examples a td td colspan 5 align center a href https software.intel.com content www us en develop documentation get started with ai linux top.html intel oneapi ai analytics toolkit a td tr tbody thead tr th colspan 9 python based apis th tr thead tbody tr td colspan 2 align center a href . docs source quantization.md quantization a td td colspan 3 align center a href . docs source mixed precision.md advanced mixed precision a td td colspan 2 align center a href . docs source pruning.md pruning sparsity a td td colspan 2 align center a href . docs source distillation.md distillation a td tr tr td colspan 2 align center a href . docs source orchestration.md orchestration a td td colspan 2 align center a href . docs source benchmark.md benchmarking a td td colspan 3 align center a href . docs source distributed.md distributed compression a td td colspan 3 align center a href . docs source export.md model export a td tr tbody thead tr th colspan 9 neural coder zero code optimization th tr thead tbody tr td colspan 1 align center a href . neural coder docs pythonlauncher.md launcher a td td colspan 2 align center a href . neural coder extensions neural compressor ext lab readme.md jupyterlab extension a td td colspan 3 align center a href . neural coder extensions neural compressor ext vscode readme.md visual studio code extension a td td colspan 3 align center a href . neural coder docs supportmatrix.md supported matrix a td tr tbody thead tr th colspan 9 advanced topics th tr thead tbody tr td colspan 1 align center a href . docs source adaptor.md adaptor a td td colspan 2 align center a href . docs source tuning strategies.md strategy a td td colspan 3 align center a href . docs source distillation quantization.md distillation for quantization a td td colspan 3 align center a href . docs source smooth quant.md smoothquant td tr tbody table selected publications events post on social media adopt with tencent taco heterogeneous optimization is also key to improving ai computing power https mp.weixin.qq.com s i fqqouw7htnwxeglgnatw mar 2023 post on social media training and inference for stable diffusion intel business https www.youtube.com watch?v emcgstljaag jan 2023 blog by intel intel amx enhances ai inference performance https www.intel.com content www us en products docs accelerator engines advanced matrix extensions alibaba solution brief.html jan 2023 blog by tensorflow optimizing tensorflow for 4th gen intel xeon processors https blog.tensorflow.org 2023 01 optimizing tensorflow for 4th gen intel xeon processors.html jan 2023 neurips 2022 fast distilbert on cpus https arxiv.org abs 2211.07715 oct 2022 neurips 2022 quala minilm a quantized length adaptive minilm https arxiv.org abs 2210.17114 oct 2022 view our full publication list . docs source publication list.md . additional content release information . docs source releases info.md contribution guidelines . docs source contributing.md legal information . docs source legal information.md security policy security.md research collaborations welcome to raise any interesting research ideas on model compression techniques and feel free to reach us inc.maintainers intel.com . look forward to our collaborations on intel neural compressor!,1019,168,2023-04-04 08:28:54,2023-04-04 06:14:47,2020-07-21 23:49:56,Python (8027165)| JavaScript (474032)| TypeScript (324852)| HTML (113258)| Shell (107677)| Jupyter Notebook (56955)| SCSS (46987)| CSS (38876)| Dockerfile (1631)| Mako (494),low-precision|`pruning|`sparsity|`auto-tuning|`knowledge-distillation|`quantization|`quantization-aware-training|`post-training-quantization|`deep-learning,25,758,"Intel Neural Compressor (formerly known as Intel Low Precision Optimization Tool), targeting to provide unified APIs for network compression technologies, such as low precision quantization, sparsity, pruning, knowledge distillation, across different deep learning frameworks to pursue optimal inference performance.",0,430259,1019,Python,intel_extension_for_pytorch,,0, An open source python library supporting popular model compression techniques on all mainstream deep learning frameworks tensorflow and pytorch. The tool provides the key features typical examples and open collaborations as below support a wide range of intel hardware such as intel xeon scalable processor,AI - Computer Vision,2023-04-05 07:52:19
562727948,https://github.com/rahulunair/stable_diffusion_arc,BSD 2-Clause License,stable diffusion inference on intel arc gpus now https blog.rahul.onl posts 2022 08 12 arc dgpu linux.html that we have our arc discrete gpu setup on linux let s try to run stable diffusion model using it. the gpu we are using is an arc a770 16 gb card. a quick recap updated steps to set up arc on linux intel has now published documentation https dgpu docs.intel.com installation guides ubuntu ubuntu jammy arc.html on how to set up arc on linux. i tried it today and it works beautifully. steps to configure arc install the 5.7 oem kernel install kernel mode drivers gpu firmware install usermod drivers for compute 3d graphics and media add user to render group install oneapi 2022.3 latest as of this writeup the version of the usermod drivers that i have tested with are stable diffusion stable diffusion is a fully open source thank you stability.ai deep learning text to image and image to image model. for more information on the model checkout the wikipedia entry https en.wikipedia.org wiki stable diffusion for the same. pytorch to use pytorch on intel gpus we need to install the intel extensions for pytorch or ipex https github.com intel intel extension for pytorch . let s get the latest release for pytorch https github.com intel intel extension for pytorch releases download v1.10.200 2bgpu torch 1.10.0a0 git3d5f2d4 cp39 cp39 linux x86 64.whl and ipex https github.com intel intel extension for pytorch releases download v1.10.200 2bgpu intel extension for pytorch 1.10.200 gpu cp39 cp39 linux x86 64.whl . 1. create a conda environment with python 3.9 and install both of the wheels. 2. install diffusers library and dependencies 3. run stable diffusion we will use a model from maintained by runwayml runwayml stable diffusion v1 5 . to use the model you will have to generate https huggingface.co docs hub security tokens a user access token for the model hub. once generated we can easily download the model using diffusers api. now that we have installed all the required packages and have the user token lets try it out executing this we get the result ! . images sd pyt fp16.png as you can see the first time you run the model it takes about 35 seconds subsequent runs take about 10 seconds you can expect this number to double when using fp32. tensorflow moving on to tensorflow we have this awesome repo from divamgupta https github.com divamgupta stable diffusion tensorflow 1. create a conda environment with python 3.9 and install tensorflow and intel extension for tensorflow wheels. let s see how to run the model using pytorch first 2. install stable diffusion tensorflow package and dependencies 3. run stable diffusion running the tensorflow model is straightforward as there are no user tokens or anything like that required. executing this we get the result ! . images sd tf fp32.png as you can see the first time you run the model it takes about 60 seconds subsequent runs take about 30 seconds. one thing to note here is that for the tensorflow version we used fp32 and not fp16 as in the case of pytorch.,31,1,2022-11-11 00:35:35,2023-04-02 10:31:23,2022-11-07 05:59:56,Python (7970),intel|`intelgpu|`intel-arc|`ipex|`itex|`pytorch|`tensorflow,0,0,Stable Difussion inference on Intel Arc dGPUs,0,9489,31,Python,intel_extension_for_pytorch,,0, We will use a model from runwayml runwayml stable diffusion v1 5. Pytorch to use pytorch on intel gpus we need to install the intel extensions for pyTorch or ipex. The tensorflow version we used is fp32 and not fp16,AI - Computer Vision,2023-04-05 07:52:19
569927055,https://github.com/Stability-AI/stablediffusion,MIT License,stable diffusion version 2 ! t2i assets stable samples txt2img 768 merged 0006.png ! t2i assets stable samples txt2img 768 merged 0002.png ! t2i assets stable samples txt2img 768 merged 0005.png this repository contains stable diffusion https github.com compvis stable diffusion models trained from scratch and will be continuously updated with new checkpoints. the following list provides an overview of all currently available models. more coming soon. news march 24 2023 stable unclip 2.1 new stable diffusion finetune stable unclip 2.1 hugging face https huggingface.co stabilityai at 768x768 resolution based on sd2.1 768. this model allows for image variations and mixing operations as described in hierarchical text conditional image generation with clip latents https arxiv.org abs 2204.06125 and thanks to its modularity can be combined with other models such as karlo https github.com kakaobrain karlo . comes in two variants stable unclip l https huggingface.co stabilityai stable diffusion 2 1 unclip blob main sd21 unclip l.ckpt and stable unclip h https huggingface.co stabilityai stable diffusion 2 1 unclip blob main sd21 unclip h.ckpt which are conditioned on clip vit l and vit h image embeddings respectively. instructions are available here doc unclip.md . a public demo of sd unclip is already available at clipdrop.co stable diffusion reimagine https clipdrop.co stable diffusion reimagine december 7 2022 version 2.1 new stable diffusion model stable diffusion 2.1 v hugging face https huggingface.co stabilityai stable diffusion 2 1 at 768x768 resolution and stable diffusion 2.1 base huggingface https huggingface.co stabilityai stable diffusion 2 1 base at 512x512 resolution both based on the same number of parameters and architecture as 2.0 and fine tuned on 2.0 on a less restrictive nsfw filtering of the laion 5b https laion.ai blog laion 5b dataset. per default the attention operation of the model is evaluated at full precision when xformers is not installed. to enable fp16 which can cause numerical instabilities with the vanilla attention module on the v2.1 model run your script with attn precision fp16 python thescript.py november 24 2022 version 2.0 new stable diffusion model stable diffusion 2.0 v at 768x768 resolution. same number of parameters in the u net as 1.5 but uses openclip vit h https github.com mlfoundations open clip as the text encoder and is trained from scratch. sd 2.0 v is a so called v prediction https arxiv.org abs 2202.00512 model. the above model is finetuned from sd 2.0 base which was trained as a standard noise prediction model on 512x512 images and is also made available. added a x4 upscaling latent text guided diffusion model image upscaling with stable diffusion . new depth guided stable diffusion model depth conditional stable diffusion finetuned from sd 2.0 base . the model is conditioned on monocular depth estimates inferred via midas https github.com isl org midas and can be used for structure preserving img2img and shape conditional synthesis. ! d2i assets stable samples depth2img depth2img01.png a text guided inpainting model image inpainting with stable diffusion finetuned from sd 2.0 base . we follow the original repository https github.com compvis stable diffusion and provide basic inference scripts to sample from the models. the original stable diffusion model was created in a collaboration with compvis https arxiv.org abs 2202.00512 and runwayml https runwayml.com and builds upon the work high resolution image synthesis with latent diffusion models https ommer lab.com research latent diffusion models br robin rombach https github.com rromb andreas blattmann https github.com ablattmann dominik lorenz https github.com qp qp patrick esser https github.com pesser bj rn ommer https hci.iwr.uni heidelberg.de staff bommer br cvpr 22 oral https openaccess.thecvf.com content cvpr2022 html rombach high resolution image synthesis with latent diffusion models cvpr 2022 paper.html github https github.com compvis latent diffusion arxiv https arxiv.org abs 2112.10752 project page https ommer lab.com research latent diffusion models and many others shout outs . stable diffusion is a latent text to image diffusion model. requirements you can update an existing latent diffusion https github.com compvis latent diffusion environment by running xformers efficient attention for more efficiency and speed on gpus we highly recommended installing the xformers https github.com facebookresearch xformers library. tested on a100 with cuda 11.4. installation needs a somewhat recent version of nvcc and gcc g obtain those e.g. via then run the following compiling takes up to 30 min . upon successful installation the code will automatically default to memory efficient attention https github.com facebookresearch xformers for the self and cross attention layers in the u net and autoencoder. general disclaimer stable diffusion models are general text to image diffusion models and therefore mirror biases and mis conceptions that are present in their training data. although efforts were made to reduce the inclusion of explicit pornographic material we do not recommend using the provided weights for services or products without additional safety mechanisms and considerations. the weights are research artifacts and should be treated as such. details on the training procedure and data as well as the intended use of the model can be found in the corresponding model card https huggingface.co stabilityai stable diffusion 2 . the weights are available via the stabilityai organization at hugging face https huggingface.co stabilityai under the creativeml open rail m license license model . stable diffusion v2 stable diffusion v2 refers to a specific configuration of the model architecture that uses a downsampling factor 8 autoencoder with an 865m unet and openclip vit h 14 text encoder for the diffusion model. the sd 2 v model produces 768x768 px outputs. evaluations with different classifier free guidance scales 1.5 2.0 3.0 4.0 5.0 6.0 7.0 8.0 and 50 ddim sampling steps show the relative improvements of the checkpoints ! sd evaluation results assets model variants.jpg text to image ! txt2img stable2 assets stable samples txt2img merged 0003.png ! txt2img stable2 assets stable samples txt2img merged 0001.png stable diffusion 2 is a latent diffusion model conditioned on the penultimate text embeddings of a clip vit h 14 text encoder. we provide a reference script for sampling reference sampling script . reference sampling script this script incorporates an invisible watermarking https github.com shieldmnt invisible watermark of the outputs to help viewers identify the images as machine generated scripts tests test watermark.py . we provide the configs for the sd2 v 768px and sd2 base 512px model. first download the weights for sd2.1 v https huggingface.co stabilityai stable diffusion 2 1 and sd2.1 base https huggingface.co stabilityai stable diffusion 2 1 base . to sample from the sd2.1 v model run the following or try out the web demo ! hugging face spaces https img.shields.io badge f0 9f a4 97 20hugging 20face spaces blue https huggingface.co spaces stabilityai stable diffusion . to sample from the base model use by default this uses the ddim sampler https arxiv.org abs 2010.02502 and renders images of size 768x768 which it was trained on in 50 steps. empirically the v models can be sampled with higher guidance scales. note the inference config for all model versions is designed to be used with ema only checkpoints. for this reason use ema false is set in the configuration otherwise the code will try to switch from non ema to ema weights. enable intel extension for pytorch optimizations in text to image script if you re planning on running text to image on intel cpu try to sample an image with torchscript and intel extension for pytorch optimizations. intel extension for pytorch extends pytorch by enabling up to date features optimizations for an extra performance boost on intel hardware. it can optimize memory layout of the operators to channel last memory format which is generally beneficial for intel cpus take advantage of the most advanced instruction set available on a machine optimize operators and many more. prerequisites before running the script make sure you have all needed libraries installed. the optimization was checked on ubuntu 20.04 . install jemalloc https github.com jemalloc jemalloc numactl https linux.die.net man 8 numactl intel openmp and intel extension for pytorch . to sample from the sd2.1 v model with torchscript ipex optimizations run the following. remember to specify desired number of instances you want to run the program on more https github.com intel intel extension for pytorch blob master intel extension for pytorch cpu launch.py l48 . to sample from the base model with ipex optimizations use if you re using a cpu that supports bfloat16 consider sample from the model with bfloat16 enabled for a performance boost like so image modification with stable diffusion ! depth2img stable2 assets stable samples depth2img merged 0000.png depth conditional stable diffusion to augment the well established img2img https github.com compvis stable diffusion image modification with stable diffusion functionality of stable diffusion we provide a shape preserving stable diffusion model. note that the original method for image modification introduces significant semantic changes w.r.t. the initial image. if that is not desired download our depth conditional stable diffusion https huggingface.co stabilityai stable diffusion 2 depth model and the dpt hybrid midas model weights https github.com intel isl dpt releases download 1 0 dpt hybrid midas 501f0c75.pt place the latter in a folder midas models and sample via or this method can be used on the samples of the base model itself. for example take this sample assets stable samples depth2img old man.png generated by an anonymous discord user. using the gradio https gradio.app or streamlit https streamlit.io script depth2img.py the midas model first infers a monocular depth estimate given this input and the diffusion model is then conditioned on the relative depth output. p align center b depth2image b br img src assets stable samples depth2img d2i.gif p this model is particularly useful for a photorealistic style see the examples assets stable samples depth2img . for a maximum strength of 1.0 the model removes all pixel based information and only relies on the text prompt and the inferred monocular depth estimate. ! depth2img stable3 assets stable samples depth2img merged 0005.png classic img2img for running the classic img2img use and adapt the checkpoint and config paths accordingly. image upscaling with stable diffusion ! upscaling x4 assets stable samples upscaling merged dog.png after downloading the weights https huggingface.co stabilityai stable diffusion x4 upscaler run or for a gradio or streamlit demo of the text guided x4 superresolution model. this model can be used both on real inputs and on synthesized examples. for the latter we recommend setting a higher noise level e.g. noise level 100 . image inpainting with stable diffusion ! inpainting stable2 assets stable inpainting merged leopards.png download the sd 2.0 inpainting checkpoint https huggingface.co stabilityai stable diffusion 2 inpainting and run or for a gradio or streamlit demo of the inpainting model. this scripts adds invisible watermarking to the demo in the runwayml https github.com runwayml stable diffusion blob main scripts inpaint st.py repository but both should work interchangeably with the checkpoints configs. shout outs thanks to hugging face https huggingface.co and in particular apolin rio https github.com apolinario for support with our model releases! stable diffusion would not be possible without laion https laion.ai and their efforts to create open large scale datasets. the deepfloyd team https twitter.com deepfloydai at stability ai for creating the subset of laion 5b https laion.ai blog laion 5b dataset used to train the model. stable diffusion 2.0 uses openclip https laion.ai blog large openclip trained by romain beaumont https github.com rom1504 . our codebase for the diffusion models builds heavily on openai s adm codebase https github.com openai guided diffusion and https github.com lucidrains denoising diffusion pytorch https github.com lucidrains denoising diffusion pytorch . thanks for open sourcing! compvis https github.com compvis stable diffusion initial stable diffusion release patrick https github.com pesser s implementation https github.com runwayml stable diffusion blob main scripts inpaint st.py of the streamlit demo for inpainting. img2img is an application of sdedit https arxiv.org abs 2108.01073 by chenlin meng https cs.stanford.edu chenlin from the stanford ai lab https cs.stanford.edu ermon website . kat s implementation https github.com compvis latent diffusion pull 51 of the plms https arxiv.org abs 2202.09778 sampler and more https github.com crowsonkb k diffusion . dpmsolver https arxiv.org abs 2206.00927 integration https github.com compvis stable diffusion pull 440 by cheng lu https github.com luchengthu . facebook s xformers https github.com facebookresearch xformers for efficient attention computation. midas https github.com isl org midas for monocular depth estimation. license the code in this repository is released under the mit license. the weights are available via the stabilityai organization at hugging face https huggingface.co stabilityai and released under the creativeml open rail m license license model license. bibtex,17343,2030,2023-04-05 07:45:05,2023-04-05 07:46:28,2022-11-23 23:59:50,Python (661926),,149,80,High-Resolution Image Synthesis with Latent Diffusion Models,0,75263,17343,Python,intel_extension_for_pytorch,,0, This repository contains stable diffusion models trained from scratch and will be continuously updated with new checkpoints. The following list provides an overview of all currently available models. stable unclip 2.1 at 768x768 resolution and stable diffusion 2 1 base hugging face,HPC,2023-04-05 07:52:19
592391289,https://github.com/IntelLabs/fastRAG,Apache License 2.0,p align center img src assets fastrag header.png width 300 p h4 align center p build and explore efficient retrieval augmented generative models and applications p h4 p align center a href tophat key features key features a a href round pushpin installation installation a a href books components components a a href rocket example use cases examples a a href running how to use how to use a a href chart with upwards trend benchmarks benchmarks a p fast rag is a research framework designed to facilitate the building of retrieval augmented generative pipelines. its main goal is to make retrieval augmented generation as efficient as possible through the use of state of the art and efficient retrieval and generative models. the framework includes a variety of sparse and dense retrieval models as well as different extractive and generative information processing models. fast rag aims to provide researchers and developers with a comprehensive tool set for exploring and advancing the field of retrieval augmented generation. tophat key features retrieval augmented x a framework for developing efficient and fast retrieval augmented generative applications using the latest transformer based nlp models but not only . optimized models includes optimized models of supported pipelines with greater compute efficiency. intel optimizations tba leverage the latest optimizations developed by intel for running pipelines with maximum hardware utilization reduced latency and increased throughput using frameworks such as intel extensions for pytorch ipex https github.com intel intel extension for pytorch and intel extension for transformers https github.com intel intel extension for transformers . customizable built using haystack https github.com deepset ai haystack and huggingface. all of fastrag s components are 100 haystack compatible. round pushpin installation preliminary requirements python 3.8 pytorch in a new virtual environment run there are various dependencies based on usage books components for a short overview of the different models see models overview models.md . unique components in fastrag plaid https arxiv.org abs 2205.09707 an extremely efficient engine for late interaction retrieval. colbert https arxiv.org abs 2112.01488 a retriever used with plaid and re ranker used with dense embeddings utilizing late interaction for relevancy scoring. fusion in decoder fid https arxiv.org abs 2007.01282 a generative reader for multi document retrieval augmented tasks. stable diffusion generator https arxiv.org pdf 2112.10752.pdf a text to image generator. pluggable to any pipeline output. retrieval oriented knowledge graph construction https arxiv.org abs 2010.01057 a pipeline component for extracting named entities and creating a graph of all the entities specified in the retrieved documents with the relations between each pair of related entities. addition components retrieval augmented summarization with t5 family models such as longt5 flan t5 https arxiv.org abs 2112.07916 an encoder decoder model based on t5 with support for long input supporting summarization translation prompts. rocket example use cases efficient open domain question answering generate answers to questions answerable by using a corpus of knowledge. retrieval with fast lexical retrieval with bm25 or late interaction dense retrieval with plaid br ranking with sentence transformers or colbert br generation with fusion in decoder notebook simple generative open domain qa with bm25 and st examples simple oqda pipeline.ipynb br notebook efficient and fast odqa with plaid colbert and fid examples plaid colbert pipeline.ipynb chatgpt open domain reranking and qa use chatgpt api to both rerank the documents for any query and provide an answer to the query using the chosen documents. notebook gpt as both reranker and reader examples gpt as both reranker and reader.ipynb open domain summarization summarize topics given free text input and a corpus of knowledge. retrieval with bm25 or other retrievers br ranking with sentence transformers or other rankers br generation using summarize prompt all documents concatenated and flan t5 generative model notebook open domain summarization examples od summarization pipeline.ipynb retrieval oriented knowledge graph construction use with any retrieval pipeline to extract named entities ner and generate relation maps using relation classification model rc . notebook knowledge graph construction examples knowledge graph construction.ipynb retrieval oriented answer image generation use with any retrieval pipeline to generate a dynamic image from the answer to the query using a diffusion model. notebook answer image generation examples answer image generation.ipynb running how to use fastrag has a modular architecture that enables the user to build retrieval augmented pipelines with different components. the components are python classes that take a set of parameters. we provide multiple examples of sets of parameters used to build common pipelines the parameters are organized in yaml files in folders such as store retriever and reader all under the configuration config folder. pipeline configuration generation the pipeline is built using haystack pipeline api and is built dynamically according to the components the user is interested in. use the pipeline generation scripts generate pipeline.py script to generate a haystack pipeline which can be run by the stand alone rest server as a service see rest api rest api . here is an example of using the script to generate a pipeline with a colbert retriever an sbert reranker and an fid reader warning plaid requirements warning if gpu is needed it should be of type rtx 3090 or newer and pytorch should be installed with cuda support using running pipelines pipelines can be run inline code service notebook once initialized properly. for a concrete example see this notebook examples simple oqda pipeline.ipynb . standalone ui demos see demo demo for a script creating stand alone demos for several workflows the script creates a rest service and a ui service ready to be used. continue reading for more details on these services. serve a pipeline via a rest service one can start a rest server with a defined pipeline yaml and send queries for processing or benchmarking. a pipeline is generated according to pipeline generation pipeline configuration generation step see usage usage . run the following this will start a uvicorn server and build a pipeline as defined in the yaml file. there is support for swagger. one can observe and interact with endpoints in a simple ui by vising http localhost 8000 docs might need to forward ports locally if working on a cluster . these are the following endpoint status sanity. version project version as defined in init . query a general query used for debugging. run a demo ui define the endpoint address according to where the web server is e.g. localhost if you start the web server on the same machine and run the following creating indexes see indexing scripts scripts indexing for information about how to create different types of indexes. pre training fine tuning models we offer an array of training scripts to finetune models of your choice for various usecases. see models overview models.md for examples model descriptions and more. chart with upwards trend benchmarks benchmarks scripts and results can be found here benchmarks benchmarks . license the code is licensed under the apache 2.0 license license . disclaimer this is not an official intel product.,164,12,2023-04-03 07:09:27,2023-04-04 12:26:49,2023-01-23 16:25:35,Python (431603)| JavaScript (47324)| HTML (38113)| CSS (30077)| C++ (20023)| Jupyter Notebook (7850)| Cuda (4540)| Batchfile (799)| Makefile (635),nlp|`benchmark|`colbert|`information-retrieval|`semantic-search|`sentence-transformers|`summarization|`transformers|`diffusion|`knowledge-graph|`multi-modal|`question-answering,3,10,Efficient Retrieval Augmentation and Generation Framework,0,1100,164,Python,intel_extension_for_pytorch,,0, fast rag aims to provide researchers and developers with a comprehensive tool set for exploring and advancing the field of retrieval augmented generation. The framework includes a variety of sparse and dense retrieval models as well as different extractive and generative information processing models,AI - Natural Language Processing,2023-04-05 07:52:19
